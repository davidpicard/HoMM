defaults:
  - model: llm
  - computer: 1-gpu
  - data: redpajamas
  - logger: wandb
  - _self_

trainer:
  _target_: lightning.pytorch.Trainer
  max_steps: 1000000
  deterministic: True
  devices: ${computer.devices}
  accelerator: ${computer.accelerator}
  strategy: ${computer.strategy}
  log_every_n_steps: 50
  num_nodes: ${computer.num_nodes}
  precision: ${computer.precision}
  gradient_clip_val: 1.0
  val_check_interval: 1000
  check_val_every_n_epoch: null
  
checkpoints:
  _target_: lightning.pytorch.callbacks.ModelCheckpoint
  dirpath: ${checkpoint_dir}/${experiment_name}
  save_last: True
  save_on_train_epoch_end: True
  every_n_train_steps: 1000

progress_bar:
  _target_: lightning.pytorch.callbacks.TQDMProgressBar
  refresh_rate: ${computer.progress_bar_refresh_rate}

seed: 3407
data_dir: ${root_dir}/datasets
root_dir:  ${hydra:runtime.cwd}
checkpoint_dir: ${root_dir}/checkpoints
experiment_name_suffix: LLM
experiment_name: ${data.name}_${model.name}_${experiment_name_suffix}
load_weight_from_checkpoint: null

hydra:
  run:
    dir: outputs/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}/${experiment_name}
  job:
    chdir: true